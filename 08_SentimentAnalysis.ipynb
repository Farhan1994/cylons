{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08 SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMd56dlvtBeogtN7s+D4yVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/cylons/blob/master/08_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPVe_x8nMtPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK, Spacy, Textacy -> MANIPULATE TEXT -> NLP Pipelines \n",
        "\n",
        "# ML -> NUMBERS only \n",
        "# Approaches to CONVERT text into NUMBERS or VECTORS or TENSORS \n",
        "# y = mx + c where x is an language statement \n",
        "\n",
        "# MANIPULATED standards -> CONVINIENCE \n",
        "\n",
        "# SPECIAL SYMBOLS INSIDE OUR TEXT so that code could be used to manipulate them \n",
        "# WE AS DEVELOPERS OR NLP Engineers manipulate text and add these symbols\n",
        "# <PAD>   0   -> Neural networks are HARDCODED-> input-size cannot be change once NN is built!\n",
        "# padding helps chopping/padding different length sentences into same size\n",
        "# I am good;   Bye.   ->    [I, am, good] ; [Bye, <PAD>, <PAD>]\n",
        "# y = mx+c ;   y = w1*I + w2*am + w3*good + bias ;   y = w1*bye + w2 * <PAD> + w3 * <PAD>\n",
        "# y = w1*100 + w2*22 + w3*55 + 102;   y = w1*42 + w2* 0 + w3* 0  \n",
        "# <START> 1   -> indicates beginning of a sentence or phrase \n",
        "# example; if text was Hello World! -> 42 50 \n",
        "# Processed statement: <START> Hello World! -> 1 42 50\n",
        "# this way when multiple sentences were present, we could mark beginning of each sentence/phrase\n",
        "\n",
        "# <PAD> 0          -> 0, any weight multiplied to <PAD> will become 0! \n",
        "# <START>  1       -> Indicates beginning of sentence/phrase \n",
        "# <UNK>  2         -> your encountered a word outside dictionary \n",
        "# <UNUSED> 3       -> SPECIAL symbol that you could give special meaning to \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOl3RkrXSQiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95b88d36-0ba4-456b-c1b6-70ecfb1aa53e"
      },
      "source": [
        "# IMDB -> has a lot of movie reviews \n",
        "# POS, NEG -> our output for a Review \n",
        "# Sentiment = weights * review + bias \n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDbkhmqpUjZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e22c6016-b3d8-49dc-ccfa-762ccc7f2f49"
      },
      "source": [
        "\n",
        "# DICTIONARIES are infinite! OXFORD, CAMBRIDGE, QUEEN's, GEORGE's, millions of words!\n",
        "# constantly increasing!\n",
        "# WE HAVE TO LIMIT OUR DICTIONARY TO A FINITE SIZE! \n",
        " # assuming our entire english consists of only 10,000 words! \n",
        "# REST of the words -> <UNKNOWN>!!!!\n",
        "imdb = keras.datasets.imdb \n",
        "HP_dictionary_size = 10000\n",
        "(xtrain, ytrain), (xtest, ytest) = imdb.load_data(num_words=HP_dictionary_size)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWIWicVZWDRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d2e1fdc3-03aa-416d-a695-97d3c9aa5476"
      },
      "source": [
        "print(xtrain[2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enE6vHS7WMTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbb7b7a9-fee4-4b50-ebcf-06d25b2f3014"
      },
      "source": [
        "ytrain[:5]\n",
        "# there are 2 outputs -> 0 or 1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUgbUCYnYJ3j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52b99d15-ee66-4ce8-b582-d41b1d70eb93"
      },
      "source": [
        "word_index = imdb.get_word_index()   # DICTIONARY HAS TO BE PROVIDED BY DATASET!\n",
        "# WHOEVER BUILDS DATASET HAS TO BUILD DICTIONARY AS PER THEIR ENCODING \n",
        "word_index['bye']\n",
        "# PRECISELY OPPOSITE of what we need! This can help us with ENCODING, not decoding\n",
        "# ENCODING is already done! \n",
        "# how do we decode? -> REVERSE THE DICTIONARY!!\n",
        "# Pass it word -> GIVES you a number!\n",
        "# Our reverse dict -> PASS a number, and get word back "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Q8ER0RY2VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = { encoding:word  for word,encoding in word_index.items()   } #dictionary_comprehension"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Tgr3SFapoI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "ec11034b-b71f-4d68-935f-e4a08756ccd5"
      },
      "source": [
        "print(dictionary[0]) # EVEN THO-> the sentences begin with 1, because we used top 10,000 words, <START>,<PAD> are missing!!!"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3801d08b7ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# EVEN THO-> the sentences begin with 1, because we used top 10,000 words, <START>,<PAD> are missing!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB7aggWtawhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3eb8b319-327e-441f-9f05-781619c9e82c"
      },
      "source": [
        "# SINCE our special symbols are MISSING, we should add them to the dictionary and then reverse it!!!\n",
        "# 0-> key was giving us an error! => 0 was missing as key \n",
        "# 0 is missing\n",
        "# i need to add 4 symbols and 0 was missing anyway\n",
        "# i need to shift only 3 symbols and ADD one 1 symbol (PAD was missing, so add it directly)! \n",
        "\n",
        "word_index = {word:(encoding+3) for word,encoding in word_index.items()}\n",
        "word_index['<PAD>'] = 0\n",
        "word_index['<START>'] = 1\n",
        "word_index['<UNK>'] = 2  # unknown words\n",
        "word_index['<UNUSED>'] = 3\n",
        "dictionary = { encoding:word  for word,encoding in word_index.items()   } \n",
        "\n",
        "print(dictionary[0]) # PAD EXPECTED\n",
        "print(dictionary[1]) # START expected! \n",
        "# NOW OUR ENGLISH DICTIONARY IS READY TO DECODE THOSE MOVIEW REVIEWS "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PAD>\n",
            "<START>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bWhctMicxhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we should pass a review, and get it decoded in english!\n",
        "def decoder(review):\n",
        "  decoded_review = [ dictionary.get(word) for word in review]\n",
        "  sentence = ' '.join(decoded_review)\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLKv8crheWZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4bd67b12-0964-4e76-b30f-6f6fd7f669f2"
      },
      "source": [
        "print(decoder(xtrain[42]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> warning this review contains spoilers do not read if you don't want some points revealed to you before you watch the film br br with a cast like this you wonder whether or not the actors and actresses knew exactly what they were getting into did they see the script and say hey close encounters of the third kind was such a hit that this one can't fail ' unfortunately it does did they even think to check on the director's <UNK> i mean would you do a movie with the director of a movie called <UNK> <UNK> ' <UNK> clark who would later go on to direct the infamous final justice ' made this it makes you wonder how the people of mystery science theater 3000 could hammer final <UNK> and completely miss out on the return ' br br the film is set in a small town in new mexico a little boy and girl are in the street <UNK> one night when a powerful <UNK> <UNK> er a spaceship appears and <UNK> over them in probably the worst special effect sequence of the film the ship <UNK> some kind of red <UNK> on them it looked like clark had held a <UNK> of water in from of the camera lens and <UNK> his <UNK> pen in it so right away you are treated with cheese anyhow the ship leaves and the adults don't believe the children elsewhere we see vincent <UNK> whom i find to be a terrific actor watch his scenes in <UNK> for proof as they are outstanding who is playing a <UNK> or as i called him the <UNK> <UNK> he steps out of the cave he is in and he and his dog are <UNK> by the ship twenty five years go by and the girl has grown up to be <UNK> shepherd who works with her father raymond <UNK> in studying unusual weather <UNK> or something like that shepherd spots some strange <UNK> in satellite pictures over that little new mexico town and she travels there to research it once she gets there the local <UNK> <UNK> her and blame her for the recent slew of cattle <UNK> that have been going on and deputy jan michael vincent comes to her rescue from this point on the film really drags as the two quickly fall for each other especially after vincent <UNK> off the locals and informs shepherd that he was the little boy that saw the ship with her twenty five years earlier while this boring mess is happening vincent <UNK> with his killer dog at his side is walking around killing the cattle and any people he runs into with an unusual item you know those glowing plastic sticks stores sell for trick or <UNK> at halloween the kind that you shake to make them glow <UNK> uses what looks like one of those glow sticks to burn <UNK> in people it's the second worst effect in the movie every time <UNK> is on screen with the glow stick the <UNK> atmosphere suddenly turns dark like the filmmakers thought the glow stick needed that <UNK> it ends up making the movie look even cheaper than it is br br and what does all this lead up to it's hard to tell when the final confusing scene arrives see <UNK> and his team of scientists try to explain the satellite images that shepherd found as some kind of calling card ' but none of it makes sense why do shepherd and vincent age and <UNK> does not <UNK> explains why he is killing cattle and people and why he wants shepherd dead but even that doesn't make much sense when you really think about it i mean why doesn't he kill jan michael vincent after all he had twenty five years to do it and the aliens won't need him if shepherd is dead anyhow so why try to kill her speaking of the aliens it is never clear what they really wanted out of shepherd and vincent what is their goal why do they wait so long to <UNK> how could they be so sure shepherd would come back not that the answer to any of these and other questions would have made the <UNK> any more pleasant you would still have bad lines really bad acting particularly by shepherd cheesy effects and poor direction luckily the stars escaped from this movie <UNK> shepherd soon went on to star in <UNK> with bruce willis jan michael vincent went on to be featured in dozens of b movies often in over the top parts raymond <UNK> made a pile of perry mason television movies right up until his death vincent <UNK> went on to be a great character actor in a huge number of films martin <UNK> who played a <UNK> law <UNK> officer quickly made the terrific alone in the <UNK> and the awful the <UNK> before rolling into the films he has been famous for recently you can bet none of these stars ever want their careers to return to the return ' <UNK> score 2 out of 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwGtQMcIecLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "ec5f47fb-814e-4b15-f6d9-e521ca169d7a"
      },
      "source": [
        "sentiment = {1:'POSITIVE', 0:'NEGATIVE'}\n",
        "for i in range(5):\n",
        "  print(sentiment.get(ytrain[i]) + ' : ' + decoder(xtrain[i]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POSITIVE : <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "NEGATIVE : <START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
            "NEGATIVE : <START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\n",
            "POSITIVE : <START> the <UNK> <UNK> at storytelling the traditional sort many years after the event i can still see in my <UNK> eye an elderly lady my friend's mother retelling the battle of <UNK> she makes the characters come alive her passion is that of an eye witness one to the events on the <UNK> heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and <UNK> of scotland as i discussed it with a friend one night in <UNK> a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional <UNK> fact and fiction blend with <UNK> role models warning stories <UNK> magic and mystery br br my name is <UNK> like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the <UNK> wonder of scotland its rugged mountains <UNK> in <UNK> the stuff of legend yet <UNK> is <UNK> in reality this is what gives it its special charm it has a rough beauty and authenticity <UNK> with some of the finest <UNK> singing you will ever hear br br <UNK> <UNK> visits his grandfather in hospital shortly before his death he burns with frustration part of him <UNK> to be in the twenty first century to hang out in <UNK> but he is raised on the western <UNK> among a <UNK> speaking community br br yet there is a deeper conflict within him he <UNK> to know the truth the truth behind his <UNK> ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last <UNK> journey to the <UNK> of one of <UNK> most <UNK> mountains can the truth be told or is it all in stories br br in this story about stories we <UNK> bloody battles <UNK> lovers the <UNK> of old and the sometimes more <UNK> <UNK> of accepted truth in doing so we each connect with <UNK> as he lives the story of his own life br br <UNK> the <UNK> <UNK> is probably the most honest <UNK> and genuinely beautiful film of scotland ever made like <UNK> i got slightly annoyed with the <UNK> of hanging stories on more stories but also like <UNK> i <UNK> this once i saw the <UNK> picture ' forget the box office <UNK> of braveheart and its like you might even <UNK> the <UNK> famous <UNK> of the wicker man to see a film that is true to scotland this one is probably unique if you maybe <UNK> on it deeply enough you might even re <UNK> the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\n",
            "NEGATIVE : <START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the <UNK> and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfYXFjLaiPkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "71d50372-cdd1-4740-cceb-5bd5409bbc78"
      },
      "source": [
        "# BEFORE BUILDING Our NN, let's PAD/CHOP these sentences to ensure they are of \n",
        "# same length\n",
        "\n",
        "xtrain_padded = keras.preprocessing.sequence.pad_sequences(xtrain, value=0, padding='post',\n",
        "                                                           truncating='post', maxlen=256)\n",
        "xtest_padded = keras.preprocessing.sequence.pad_sequences(xtest, value=0, padding='post', \n",
        "                                                          truncating='post', maxlen=256)\n",
        "print(decoder(xtrain[42]))\n",
        "print(decoder(xtrain_padded[42])) # LONG SENTENCE CHOPPED\n",
        "print(decoder(xtrain[62])) # SHORT SENTENCE PADDED\n",
        "print(decoder(xtrain_padded[62]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> warning this review contains spoilers do not read if you don't want some points revealed to you before you watch the film br br with a cast like this you wonder whether or not the actors and actresses knew exactly what they were getting into did they see the script and say hey close encounters of the third kind was such a hit that this one can't fail ' unfortunately it does did they even think to check on the director's <UNK> i mean would you do a movie with the director of a movie called <UNK> <UNK> ' <UNK> clark who would later go on to direct the infamous final justice ' made this it makes you wonder how the people of mystery science theater 3000 could hammer final <UNK> and completely miss out on the return ' br br the film is set in a small town in new mexico a little boy and girl are in the street <UNK> one night when a powerful <UNK> <UNK> er a spaceship appears and <UNK> over them in probably the worst special effect sequence of the film the ship <UNK> some kind of red <UNK> on them it looked like clark had held a <UNK> of water in from of the camera lens and <UNK> his <UNK> pen in it so right away you are treated with cheese anyhow the ship leaves and the adults don't believe the children elsewhere we see vincent <UNK> whom i find to be a terrific actor watch his scenes in <UNK> for proof as they are outstanding who is playing a <UNK> or as i called him the <UNK> <UNK> he steps out of the cave he is in and he and his dog are <UNK> by the ship twenty five years go by and the girl has grown up to be <UNK> shepherd who works with her father raymond <UNK> in studying unusual weather <UNK> or something like that shepherd spots some strange <UNK> in satellite pictures over that little new mexico town and she travels there to research it once she gets there the local <UNK> <UNK> her and blame her for the recent slew of cattle <UNK> that have been going on and deputy jan michael vincent comes to her rescue from this point on the film really drags as the two quickly fall for each other especially after vincent <UNK> off the locals and informs shepherd that he was the little boy that saw the ship with her twenty five years earlier while this boring mess is happening vincent <UNK> with his killer dog at his side is walking around killing the cattle and any people he runs into with an unusual item you know those glowing plastic sticks stores sell for trick or <UNK> at halloween the kind that you shake to make them glow <UNK> uses what looks like one of those glow sticks to burn <UNK> in people it's the second worst effect in the movie every time <UNK> is on screen with the glow stick the <UNK> atmosphere suddenly turns dark like the filmmakers thought the glow stick needed that <UNK> it ends up making the movie look even cheaper than it is br br and what does all this lead up to it's hard to tell when the final confusing scene arrives see <UNK> and his team of scientists try to explain the satellite images that shepherd found as some kind of calling card ' but none of it makes sense why do shepherd and vincent age and <UNK> does not <UNK> explains why he is killing cattle and people and why he wants shepherd dead but even that doesn't make much sense when you really think about it i mean why doesn't he kill jan michael vincent after all he had twenty five years to do it and the aliens won't need him if shepherd is dead anyhow so why try to kill her speaking of the aliens it is never clear what they really wanted out of shepherd and vincent what is their goal why do they wait so long to <UNK> how could they be so sure shepherd would come back not that the answer to any of these and other questions would have made the <UNK> any more pleasant you would still have bad lines really bad acting particularly by shepherd cheesy effects and poor direction luckily the stars escaped from this movie <UNK> shepherd soon went on to star in <UNK> with bruce willis jan michael vincent went on to be featured in dozens of b movies often in over the top parts raymond <UNK> made a pile of perry mason television movies right up until his death vincent <UNK> went on to be a great character actor in a huge number of films martin <UNK> who played a <UNK> law <UNK> officer quickly made the terrific alone in the <UNK> and the awful the <UNK> before rolling into the films he has been famous for recently you can bet none of these stars ever want their careers to return to the return ' <UNK> score 2 out of 10\n",
            "<START> warning this review contains spoilers do not read if you don't want some points revealed to you before you watch the film br br with a cast like this you wonder whether or not the actors and actresses knew exactly what they were getting into did they see the script and say hey close encounters of the third kind was such a hit that this one can't fail ' unfortunately it does did they even think to check on the director's <UNK> i mean would you do a movie with the director of a movie called <UNK> <UNK> ' <UNK> clark who would later go on to direct the infamous final justice ' made this it makes you wonder how the people of mystery science theater 3000 could hammer final <UNK> and completely miss out on the return ' br br the film is set in a small town in new mexico a little boy and girl are in the street <UNK> one night when a powerful <UNK> <UNK> er a spaceship appears and <UNK> over them in probably the worst special effect sequence of the film the ship <UNK> some kind of red <UNK> on them it looked like clark had held a <UNK> of water in from of the camera lens and <UNK> his <UNK> pen in it so right away you are treated with cheese anyhow the ship leaves and the adults don't believe the children elsewhere we see vincent <UNK> whom i find to be a terrific actor watch his scenes\n",
            "<START> i found this movie to be a great idea that didn't deliver it seems they found a way to build suspense but couldn't stage their <UNK> very well in one case the police are on the clock to find the <UNK> of the <UNK> they <UNK> go from dentist to dentist to match a dental record at the same time the kidnapped man mason escapes through the elevator <UNK> after all the build up the police arrive at the same time he gets free which is very anti climatic to say the least there are also large narration scenes that take us inside the thinking of the <UNK> husband and wife which <UNK> from the suspense rather than adds to it we are fully aware of their tension and the voice over is an insult and <UNK> the viewer of any chance of a personal experience with the fear as hitchcock proved time and again is far more effective the greatest disappointment is to sit through the whole movie and the get the quick rather bland ending i mean it just ends in a <UNK>\n",
            "<START> i found this movie to be a great idea that didn't deliver it seems they found a way to build suspense but couldn't stage their <UNK> very well in one case the police are on the clock to find the <UNK> of the <UNK> they <UNK> go from dentist to dentist to match a dental record at the same time the kidnapped man mason escapes through the elevator <UNK> after all the build up the police arrive at the same time he gets free which is very anti climatic to say the least there are also large narration scenes that take us inside the thinking of the <UNK> husband and wife which <UNK> from the suspense rather than adds to it we are fully aware of their tension and the voice over is an insult and <UNK> the viewer of any chance of a personal experience with the fear as hitchcock proved time and again is far more effective the greatest disappointment is to sit through the whole movie and the get the quick rather bland ending i mean it just ends in a <UNK> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pugXXVlj3E_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HP_dictionary_size = 10000\n",
        "HP_dim_from_embed = 16 \n",
        "# each word broken into 16 vectors! total= 10,000 X 16 weights for vectors\n",
        "HP_dim_l3_l4 = 64\n",
        "HP_dim_l3_l3extra = 128\n",
        "HP_epoch = 50\n",
        "HP_maxlen = 256\n",
        "HP_batch_size = 128   # batch size is HOW many elements to read in 1 go! \n",
        "# SPEEDS up the learning by reading multiple inputs at a time! \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t5r0Lvk4Y63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "51d51eae-ea80-47df-98fa-39d880360acf"
      },
      "source": [
        "# this didn't work because layers got copied in SHALLOW not deep\n",
        "# as a result, in the list, they were still attached to each other\n",
        "# while creating network 3, it actually got associated with prev networks because of layer\n",
        "# names!!\n",
        "\n",
        "layer1 = keras.layers.Embedding(HP_dictionary_size, HP_dim_from_embed)\n",
        "layer2 = keras.layers.GlobalAveragePooling1D()\n",
        "layer3 = keras.layers.Dense(HP_dim_l3_l4, activation=tf.nn.relu)\n",
        "layer3_alternate = keras.layers.Dense(HP_dim_l3_l4)\n",
        "layer3_extra = keras.layers.Dense(HP_dim_l3_l3extra, activation=tf.nn.relu)\n",
        "layer4 = keras.layers.Dense(1, activation = tf.nn.sigmoid)\n",
        "\n",
        "model_layers1 = [layer1, layer2, layer3, layer4]\n",
        "model_layers2 = [layer1, layer2, layer3_alternate, layer4]\n",
        "model_layers3 = [layer1, layer2, layer3, layer3_extra, layer4]\n",
        "\n",
        "m1 = keras.Sequential(model_layers1)\n",
        "m2 = keras.Sequential(model_layers2)\n",
        "m3 = keras.Sequential(model_layers3)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-0a34e95d1a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_layers1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_layers2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mm3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_layers3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 819\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;34m' incompatible with the layer: expected axis '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;34m' of input shape to have value '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 ' but received input with shape ' + str(shape))\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;31m# Check shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer dense_6 is incompatible with the layer: expected axis -1 of input shape to have value 64 but received input with shape [None, 128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyZFxDfu7Taq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "layer1 = keras.layers.Embedding(HP_dictionary_size, HP_dim_from_embed)\n",
        "layer2 = keras.layers.GlobalAveragePooling1D()\n",
        "layer3 = keras.layers.Dense(HP_dim_l3_l4, activation=tf.nn.relu)\n",
        "layer4 = keras.layers.Dense(1, activation = tf.nn.sigmoid)\n",
        "\n",
        "model_layers1 = [layer1, layer2, layer3, layer4]\n",
        "m1 = keras.Sequential(model_layers1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5CLRfDh7TLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "layer1_b = keras.layers.Embedding(HP_dictionary_size, HP_dim_from_embed)\n",
        "layer2_b = keras.layers.GlobalAveragePooling1D()\n",
        "layer3_b = keras.layers.Dense(HP_dim_l3_l4)\n",
        "layer4_b = keras.layers.Dense(1, activation = tf.nn.sigmoid)\n",
        "\n",
        "model_layers2 = [layer1_b, layer2_b, layer3_b, layer4_b]\n",
        "m2 = keras.Sequential(model_layers2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB7mMN1l9Rzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer1_c = keras.layers.Embedding(HP_dictionary_size, HP_dim_from_embed)\n",
        "layer2_c = keras.layers.GlobalAveragePooling1D()\n",
        "layer3_c = keras.layers.Dense(HP_dim_l3_l4, activation=tf.nn.relu)\n",
        "layer4_c = keras.layers.Dense(128, activation=tf.nn.relu)\n",
        "layer5_c = keras.layers.Dense(1, activation = tf.nn.sigmoid)\n",
        "\n",
        "model_layers3 = [layer1_c, layer2_c, layer3_c, layer4_c, layer5_c]\n",
        "m3 = keras.Sequential(model_layers3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm2JDFyU9kXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "71e7f530-8a11-4f23-b701-63ad76c79cdf"
      },
      "source": [
        "m1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "m1.summary()\n",
        "# 10,000 words X 16 vectors = 160,000 weights \n",
        "# To calc average -> SUM/N -> No weights required! 0 params\n",
        "# 16 X 64 + 64 = 1088\n",
        "# 64 X 1 + 1 = 65"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                1088      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 161,153\n",
            "Trainable params: 161,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40QGKeYe9kOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d8a954cc-007e-4b98-b56c-c2cefee243fe"
      },
      "source": [
        "m2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "m2.summary()\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_3 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                1088      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 161,153\n",
            "Trainable params: 161,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTobegJE9j_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "513da55e-3b09-4a40-ba7a-46f31d8c811b"
      },
      "source": [
        "\n",
        "m3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "m3.summary()\n",
        "# dense layers:\n",
        "# parameters = input_dim(Weights from previous layers!!!) X weights + bias \n",
        "# 16X64 + 64 = 1088\n",
        "# 64X128 + 128 = 8320\n",
        "# 128X1 + 1 = 129"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_4 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                1088      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 169,537\n",
            "Trainable params: 169,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPf8N0cZY9bF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c4587c55-7486-4745-b85d-a558339b3ec6"
      },
      "source": [
        "# last time, we had taken Validation_split to get some validation data!\n",
        "# today, let's extract our validation data \n",
        "print(len(xtrain))\n",
        "\n",
        "print(len(xtest))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FypEFg8ebO8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation data -> divided testing into (25,000) -> val (10,000) + test (15,000)\n",
        "\n",
        "xval = xtest_padded[:10000]\n",
        "xtest_padded_reduced = xtest_padded[10000:]\n",
        "yval = ytest[:10000]\n",
        "ytest_reduced = ytest[10000:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD8BfKYldvVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee691a27-df65-4b16-8203-d6a1b25638b4"
      },
      "source": [
        "# xtrain_padded, xtest_padded_reduced\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "history_m1 = m1.fit(xtrain_padded, ytrain, epochs=HP_epoch, batch_size=HP_batch_size, \n",
        "                    validation_data = (xval, yval), verbose=1) \n",
        "end_time = time.time()\n",
        "print('Time Taken = ' + str(end_time-start_time))\n",
        "# OBSERVE-> BATCH_Size is being read per set (previous 1 input 1 time)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0667 - acc: 0.9832 - val_loss: 0.5637 - val_acc: 0.8410\n",
            "Epoch 2/50\n",
            "25000/25000 [==============================] - 2s 69us/sample - loss: 0.0599 - acc: 0.9851 - val_loss: 0.6038 - val_acc: 0.8425\n",
            "Epoch 3/50\n",
            "25000/25000 [==============================] - 2s 70us/sample - loss: 0.0549 - acc: 0.9869 - val_loss: 0.6293 - val_acc: 0.8401\n",
            "Epoch 4/50\n",
            "25000/25000 [==============================] - 2s 68us/sample - loss: 0.0478 - acc: 0.9894 - val_loss: 0.6681 - val_acc: 0.8376\n",
            "Epoch 5/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 0.0432 - acc: 0.9916 - val_loss: 0.7013 - val_acc: 0.8332\n",
            "Epoch 6/50\n",
            "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0387 - acc: 0.9926 - val_loss: 0.7429 - val_acc: 0.8362\n",
            "Epoch 7/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0345 - acc: 0.9936 - val_loss: 0.7709 - val_acc: 0.8336\n",
            "Epoch 8/50\n",
            "25000/25000 [==============================] - 2s 68us/sample - loss: 0.0309 - acc: 0.9947 - val_loss: 0.8147 - val_acc: 0.8305\n",
            "Epoch 9/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0271 - acc: 0.9957 - val_loss: 0.8534 - val_acc: 0.8314\n",
            "Epoch 10/50\n",
            "25000/25000 [==============================] - 2s 68us/sample - loss: 0.0237 - acc: 0.9964 - val_loss: 0.8885 - val_acc: 0.8268\n",
            "Epoch 11/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 0.0218 - acc: 0.9968 - val_loss: 0.9388 - val_acc: 0.8297\n",
            "Epoch 12/50\n",
            "25000/25000 [==============================] - 2s 67us/sample - loss: 0.0182 - acc: 0.9974 - val_loss: 0.9821 - val_acc: 0.8263\n",
            "Epoch 13/50\n",
            "25000/25000 [==============================] - 2s 75us/sample - loss: 0.0161 - acc: 0.9978 - val_loss: 1.0237 - val_acc: 0.8269\n",
            "Epoch 14/50\n",
            "25000/25000 [==============================] - 2s 68us/sample - loss: 0.0148 - acc: 0.9982 - val_loss: 1.0689 - val_acc: 0.8246\n",
            "Epoch 15/50\n",
            "25000/25000 [==============================] - 2s 74us/sample - loss: 0.0128 - acc: 0.9982 - val_loss: 1.1130 - val_acc: 0.8242\n",
            "Epoch 16/50\n",
            "25000/25000 [==============================] - 2s 69us/sample - loss: 0.0107 - acc: 0.9988 - val_loss: 1.1600 - val_acc: 0.8232\n",
            "Epoch 17/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 0.0088 - acc: 0.9991 - val_loss: 1.2002 - val_acc: 0.8208\n",
            "Epoch 18/50\n",
            "25000/25000 [==============================] - 2s 69us/sample - loss: 0.0075 - acc: 0.9993 - val_loss: 1.2505 - val_acc: 0.8210\n",
            "Epoch 19/50\n",
            "25000/25000 [==============================] - 2s 74us/sample - loss: 0.0066 - acc: 0.9992 - val_loss: 1.3008 - val_acc: 0.8193\n",
            "Epoch 20/50\n",
            "25000/25000 [==============================] - 2s 68us/sample - loss: 0.0051 - acc: 0.9997 - val_loss: 1.3593 - val_acc: 0.8160\n",
            "Epoch 21/50\n",
            "25000/25000 [==============================] - 2s 75us/sample - loss: 0.0039 - acc: 0.9998 - val_loss: 1.4163 - val_acc: 0.8191\n",
            "Epoch 22/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 0.0028 - acc: 0.9999 - val_loss: 1.4718 - val_acc: 0.8190\n",
            "Epoch 23/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 1.5225 - val_acc: 0.8188\n",
            "Epoch 24/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 1.5662 - val_acc: 0.8180\n",
            "Epoch 25/50\n",
            "25000/25000 [==============================] - 2s 74us/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 1.6060 - val_acc: 0.8183\n",
            "Epoch 26/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 1.6550 - val_acc: 0.8175\n",
            "Epoch 27/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.7133 - val_acc: 0.8172\n",
            "Epoch 28/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 1.7365 - val_acc: 0.8173\n",
            "Epoch 29/50\n",
            "25000/25000 [==============================] - 2s 76us/sample - loss: 8.4276e-04 - acc: 1.0000 - val_loss: 1.7720 - val_acc: 0.8173\n",
            "Epoch 30/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 7.1319e-04 - acc: 1.0000 - val_loss: 1.8167 - val_acc: 0.8173\n",
            "Epoch 31/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 6.6582e-04 - acc: 1.0000 - val_loss: 1.8364 - val_acc: 0.8171\n",
            "Epoch 32/50\n",
            "25000/25000 [==============================] - 2s 76us/sample - loss: 5.3610e-04 - acc: 1.0000 - val_loss: 1.8833 - val_acc: 0.8163\n",
            "Epoch 33/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 4.6584e-04 - acc: 1.0000 - val_loss: 1.9164 - val_acc: 0.8163\n",
            "Epoch 34/50\n",
            "25000/25000 [==============================] - 2s 70us/sample - loss: 4.1374e-04 - acc: 1.0000 - val_loss: 1.9483 - val_acc: 0.8166\n",
            "Epoch 35/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 3.5277e-04 - acc: 1.0000 - val_loss: 1.9872 - val_acc: 0.8166\n",
            "Epoch 36/50\n",
            "25000/25000 [==============================] - 2s 76us/sample - loss: 3.2581e-04 - acc: 1.0000 - val_loss: 2.0217 - val_acc: 0.8163\n",
            "Epoch 37/50\n",
            "25000/25000 [==============================] - 2s 74us/sample - loss: 2.8451e-04 - acc: 1.0000 - val_loss: 2.0371 - val_acc: 0.8152\n",
            "Epoch 38/50\n",
            "25000/25000 [==============================] - 2s 70us/sample - loss: 2.7632e-04 - acc: 1.0000 - val_loss: 2.1052 - val_acc: 0.8149\n",
            "Epoch 39/50\n",
            "25000/25000 [==============================] - 2s 74us/sample - loss: 2.1387e-04 - acc: 1.0000 - val_loss: 2.1188 - val_acc: 0.8165\n",
            "Epoch 40/50\n",
            "25000/25000 [==============================] - 2s 74us/sample - loss: 2.1408e-04 - acc: 1.0000 - val_loss: 2.1547 - val_acc: 0.8157\n",
            "Epoch 41/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 2.0986e-04 - acc: 1.0000 - val_loss: 2.1908 - val_acc: 0.8156\n",
            "Epoch 42/50\n",
            "25000/25000 [==============================] - 2s 68us/sample - loss: 1.5522e-04 - acc: 1.0000 - val_loss: 2.2225 - val_acc: 0.8147\n",
            "Epoch 43/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 1.2997e-04 - acc: 1.0000 - val_loss: 2.2617 - val_acc: 0.8151\n",
            "Epoch 44/50\n",
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.0053 - acc: 0.9982 - val_loss: 2.3177 - val_acc: 0.8134\n",
            "Epoch 45/50\n",
            "25000/25000 [==============================] - 2s 70us/sample - loss: 0.0353 - acc: 0.9875 - val_loss: 2.3350 - val_acc: 0.8137\n",
            "Epoch 46/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 0.0117 - acc: 0.9957 - val_loss: 2.3459 - val_acc: 0.8151\n",
            "Epoch 47/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 0.0021 - acc: 0.9996 - val_loss: 2.2705 - val_acc: 0.8136\n",
            "Epoch 48/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 5.5176e-04 - acc: 1.0000 - val_loss: 2.2899 - val_acc: 0.8153\n",
            "Epoch 49/50\n",
            "25000/25000 [==============================] - 2s 71us/sample - loss: 3.8184e-04 - acc: 1.0000 - val_loss: 2.2955 - val_acc: 0.8156\n",
            "Epoch 50/50\n",
            "25000/25000 [==============================] - 2s 73us/sample - loss: 3.2953e-04 - acc: 1.0000 - val_loss: 2.3109 - val_acc: 0.8157\n",
            "Time Taken = 89.63974213600159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZePbVGqOdvHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "history_m2 = m2.fit(xtrain_padded, ytrain, epochs=HP_epoch, batch_size=HP_batch_size, \n",
        "                    validation_data = (xval, yval), verbose=0)\n",
        "end_time = time.time()\n",
        "print('Time Taken = ' + str(end_time-start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz-d0F7Kblme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "8c967b2e-647d-4641-9f27-b47e37299d9a"
      },
      "source": [
        "\n",
        "start_time = time.time()\n",
        "history_m3 = m3.fit(xtrain_padded, ytrain, epochs=HP_epoch, batch_size=HP_batch_size, \n",
        "                    validation_data = (xval, yval), verbose=0)\n",
        "end_time = time.time()\n",
        "print('Time Taken = ' + str(end_time-start_time))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-4aee888ea4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m history_m1 = m1.fit(xtrain_padded, ytrain, epochs=HP_epoch, batch_size=HP_batch_size, \n\u001b[0;32m----> 5\u001b[0;31m                     validation_data = (xval, yval), verbose=0) # no output \n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time Taken = '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15XFsIRKj6yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# it depends on context usually \n",
        "# where you get sentiments on that review may be it comes afetr 100 words?\n",
        "# \n",
        "# REASON why we are having a limit of 256\n",
        "# if even after 256 words somebody is not able to give me a review, then what use was it?\n",
        "# it was pure blabbering then!\n",
        "#\n",
        "# what about sarcastic reviews?\n",
        "# ALL DEPENDS on your training data! 0,1,2 -> NEG, POS, SARCASM \n",
        "# and need enough training data and labels to have NN learn this \n",
        "\n",
        "# How to build formulae\n",
        "# https://www.tutorialspoint.com/discrete_mathematics/discrete_mathematical_induction.htm\n",
        "# Ralph L Grimaldi's Discrete and Combinatorial math "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}